本周主要学习笔记：
1、eval 和exec
  区别：有无返回值
2、生成器和迭代器
  区别：迭代器是所有内容都在内容中，使用next函数依次遍历。
        生成器不会把内容放在内存中，每次调用next的时候返回的都是每次计算的结果，用完之后立刻销毁。
  生成器函数：
    yield关键字实现
3、工厂函数
  利用现象对象的思想，传入不同的参数输出不同的结果
  利用偏函数实现工厂函数
    def f1（*args，**kwargs）:
      pass
    def f2():
      return f1(*,**)
    def f3():
      return f1(*,**)

  闭合函数/闭包函数
    函数嵌套，外部作用域变量，返回值内层函数
    比如：
    def outer(f):
      def inner(*,**):
        pass
      return inner
  装饰器：本质是一个闭包函数，
    在原有函数的外面包装一层函数，是新函数在返回原有函数之前实现一些功能。
    重点：多个装饰器的执行顺序（在之前的内容中一直不明白为什么装饰器的执行的顺序很奇怪，下面的一句话即可解决疑惑。）
      装饰器的加载顺序是从下往上，在调用时，执行函数顺序是从下往上的。
4、元类METAClass
  type 动态的创建简单的类
  创建复杂的类需要使用元类
  参考代码：metaclass.py
5、机器学习（损失函数）
  求解机器学习模型参数最优解问题：最小二乘法、梯度下降
  最小二乘法思想：
    一种统计学习优化技术，他的目标是最小化误差平方之和来作为目标，从而找到最优模型，这个模型可以拟合观测数据。
    回归学习中，常用损失函数是平方损失函数，在此情况下，回归问题可以用著名的最小二乘法来解决。
    目标公式：
      J = ∑（f(x) - Y）
      其中 f为模型值，Y为观测值
    最小二乘法就是使得J的值最小化。
    最小二乘法利用是把目标函数化为矩阵运算问题，然后求导使其等于0，从而求得极值。
  随机梯度下降：
    思路：对参数向量求导，使得梯度为0，然后得到参数变量的迭代更新公式。
    W ： = W - a*参数偏导
  二者区别：
    最小二乘法是直接对\Delta求导找出全局最小，是非迭代法。而梯度下降法是一种迭代法，先给定一个\beta ，
    然后向\Delta下降最快的方向调整\beta ，在若干次迭代之后找到局部最小。梯度下降法的缺点是到最小点的时候收敛速度变慢，
    并且对初始点的选择极为敏感。
